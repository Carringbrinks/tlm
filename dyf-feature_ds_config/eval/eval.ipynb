{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-16 12:41:47,248] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PretrainedConfig, AutoConfig, PreTrainedModel\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "class TimeLLMConfig(PretrainedConfig):\n",
    "    \"\"\"\n",
    "    HF-Compatible Configuration for TimeLLM Model\n",
    "    (Non-dataclass version with full PretrainedConfig integration)\n",
    "    \"\"\"\n",
    "    model_type = \"time_llm\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_kwargs: Dict[str, Any] = None,\n",
    "        prediction_length: int = 24,\n",
    "        n_tokens: int = 4096,\n",
    "        query_len: int = 36,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # 必须调用父类初始化（处理HF标准参数）\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # 核心自定义参数\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs or {}\n",
    "        self.prediction_length = prediction_length\n",
    "        self.n_tokens = n_tokens\n",
    "        self.query_len = query_len\n",
    "\n",
    "    def create_tokenizer(self) -> 'TimeLLMTokenizer':\n",
    "        \n",
    "        return MeanScaleQuantileBins(**self.tokenizer_kwargs, config=self)\n",
    "\n",
    "class TimeLLMTokenizer:\n",
    "    \"\"\"Base class for time series tokenizers\"\"\"\n",
    "    def context_input_transform(self, context: torch.Tensor) -> Tuple:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def label_input_transform(self, label: torch.Tensor, tokenizer_state: Any) -> Tuple:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def output_transform(self, samples: torch.Tensor, tokenizer_state: Any) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MeanScaleQuantileBins(TimeLLMTokenizer):\n",
    "    \"\"\"Quantile-based binning tokenizer for time series\"\"\"\n",
    "    def __init__(self, low_limit: float, high_limit: float, config: TimeLLMConfig):\n",
    "        self.config = config\n",
    "        self.centers = torch.linspace(\n",
    "            low_limit, high_limit,\n",
    "            config.n_tokens - 1,\n",
    "        )\n",
    "        self.boundaries = torch.concat([\n",
    "            torch.tensor([-1e20]),\n",
    "            (self.centers[1:] + self.centers[:-1]) / 2,\n",
    "            torch.tensor([1e20])\n",
    "        ])\n",
    "\n",
    "    def _input_transform(self, context: torch.Tensor, scale: Optional[torch.Tensor] = None):\n",
    "        context = context.float()\n",
    "        attention_mask = ~torch.isnan(context)\n",
    "\n",
    "        if scale is None:\n",
    "            scale = torch.nansum(torch.abs(context) * attention_mask, dim=-1) / \\\n",
    "                   torch.nansum(attention_mask, dim=-1)\n",
    "            scale[~(scale > 0)] = 1.0\n",
    "\n",
    "        scaled_context = context / scale.unsqueeze(-1)\n",
    "        token_ids = torch.bucketize(scaled_context, self.boundaries,right=True).clamp(0, self.config.n_tokens - 1)  # 直接使用完整token空间\n",
    "        \n",
    "        return token_ids, attention_mask, scale\n",
    "\n",
    "    def context_input_transform(self, context: torch.Tensor):\n",
    "        # if context.shape[-1] > self.config.context_length:\n",
    "        #     context = context[..., -self.config.context_length:]\n",
    "            \n",
    "        token_ids, attention_mask, scale = self._input_transform(context)\n",
    "            \n",
    "        return token_ids, attention_mask, scale\n",
    "\n",
    "    def output_transform(self, samples: torch.Tensor, scale: torch.Tensor):\n",
    "        \"\"\"将模型输出的token索引转换为实际数值\"\"\"\n",
    "        indices = torch.clamp(\n",
    "            samples,  # 直接使用原始token索引\n",
    "            min=0,\n",
    "            max=len(self.centers)-1\n",
    "        )\n",
    "        return self.centers[indices] * scale.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class QueryAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, latent_len: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.latent_len = latent_len\n",
    "        # 可学习的Query矩阵 (L×D)\n",
    "        self.query = nn.Parameter(torch.randn(latent_len, embed_dim))\n",
    "        # 多头注意力\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        输入: \n",
    "          x: [B, T, D] \n",
    "          key_padding_mask: [B, T]（可选）\n",
    "        输出: \n",
    "          [B, L, D]\n",
    "        \"\"\"\n",
    "        # 扩展Query为[B, L, D]\n",
    "        queries = self.query.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        \n",
    "        # 注意力计算\n",
    "        attn_out, _ = self.attn(\n",
    "            query=queries,  # [B, L, D]\n",
    "            key=x,          # [B, T, D]\n",
    "            value=x,        # [B, T, D]\n",
    "            key_padding_mask=key_padding_mask  # 忽略padding部分\n",
    "        )\n",
    "        return attn_out  # [B, L, D]\n",
    "\n",
    "class TimeLLMModel(PreTrainedModel):\n",
    "    config_class = TimeLLMConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        # 2. 文本处理模块\n",
    "        self.llm_config = AutoConfig.from_pretrained(config.llm_name, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_name, trust_remote_code=True)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            config.llm_name,\n",
    "            config=self.llm_config,\n",
    "            # device_map='auto', # 必须用关键字参数\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm_dim = self.llm_config.hidden_size  # 模型的隐藏维度\n",
    "\n",
    "        # 1. 时序处理模块\n",
    "        self.bin_embed = nn.Embedding(config.n_tokens, self.llm_dim)\n",
    "        self.query_attn = QueryAttention(self.llm_dim, config.query_len)  # Query向量\n",
    "        self.alignment = nn.Sequential(\n",
    "            nn.Linear(self.llm_dim, self.llm_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.llm_dim, self.llm_dim)\n",
    "        )\n",
    "        \n",
    "    def process_ts(self, bin_id):\n",
    "        \"\"\"时序数据编码：分箱 -> Embedding -> 注意力筛选 -> 投影\"\"\"\n",
    "        # ts_data: [batch, context_length]\n",
    "        \n",
    "        bin_embedding = self.bin_embed(bin_id)  # [batch, ctx_len, dim]\n",
    "        bin_feat = self.query_attn(bin_embedding)\n",
    "        \n",
    "        return self.alignment(bin_feat)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bin_ids, labels=None, scales=None):\n",
    "        \"\"\"\n",
    "        关键设计：\n",
    "        - 输入格式：[时序Token][文本Token][预测值Token]\n",
    "        - 训练时：通过错位labels实现自回归\n",
    "        \"\"\"\n",
    "        # 1. 处理时序数据\n",
    "        ts_emb = self.process_ts(bin_ids) \n",
    "        \n",
    "        # 2. 获取文本嵌入\n",
    "        text_emb = self.llm.get_input_embeddings()(input_ids)  \n",
    "        \n",
    "        # 3. 拼接输入 [时序][文本]\n",
    "        inputs_embeds = torch.cat([ts_emb, text_emb], dim=1)  \n",
    "        \n",
    "        # 5. 通过LLM生成（自回归）\n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels  \n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    # def generate(self, bin_ids, text_input, max_new_tokens=500, eos_token_id=None):\n",
    "    #     device = next(self.parameters()).device\n",
    "        \n",
    "    #     ts_emb = self.process_ts(bin_ids.to(device))  # [batch, 36, dim]\n",
    "        \n",
    "    #     # 2. 处理文本输入（需对齐时序长度变化）\n",
    "    #     inputs = self.tokenizer(text_input, return_tensors='pt')\n",
    "    #     input_ids = inputs.input_ids.to(device)\n",
    "        \n",
    "    #     # 关键修改：调整mask初始长度\n",
    "    #     ts_len = ts_emb.shape[1]  # 36\n",
    "    #     # text_len = input_ids.shape[1]\n",
    "    #     attention_mask = torch.cat([\n",
    "    #         torch.ones(1, ts_len, device=device),  # 时序部分全1\n",
    "    #         inputs.attention_mask.to(device)       # 文本部分原始mask\n",
    "    #     ], dim=1)\n",
    "        \n",
    "    #     # 3. 增量生成\n",
    "    #     past_key_values = None\n",
    "    #     generated_ids = input_ids.clone()\n",
    "        \n",
    "    #     print(f\"时序长度: {ts_emb.shape[1]}, 文本长度: {input_ids.shape[1]}\")\n",
    "    #     print(self.llm.config.max_position_embeddings)\n",
    "        \n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         # 文本嵌入（仅最新token）\n",
    "    #         text_emb = self.llm.get_input_embeddings()(input_ids[:, -1:])\n",
    "    #         inputs_embeds = torch.cat([ts_emb, text_emb], dim=1)\n",
    "            \n",
    "    #         # 动态扩展mask（每次新增1个token）\n",
    "    #         current_mask = torch.cat([\n",
    "    #             attention_mask,\n",
    "    #             torch.ones(1, 1, device=device, dtype=torch.long)\n",
    "    #         ], dim=1)\n",
    "            \n",
    "    #         outputs = self.llm(\n",
    "    #             inputs_embeds=inputs_embeds,\n",
    "    #             attention_mask=current_mask,\n",
    "    #             past_key_values=past_key_values,\n",
    "    #             use_cache=True\n",
    "    #         )\n",
    "            \n",
    "    #         # 更新状态\n",
    "    #         past_key_values = outputs.past_key_values\n",
    "    #         next_token = outputs.logits[:, -1, :].argmax(-1)\n",
    "    #         generated_ids = torch.cat([generated_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    #         attention_mask = current_mask  # 重要！保持mask同步更新\n",
    "\n",
    "    #         # print(\"EOS token ID:\", llm_tokenizer.eos_token_id)\n",
    "    #         # print(\"首个生成token:\", next_token.item())\n",
    "            \n",
    "    #         if eos_token_id is not None and next_token.item() == eos_token_id:\n",
    "    #             break\n",
    "        \n",
    "    #     return self.tokenizer.decode(generated_ids[:, input_ids.shape[1]:][0], skip_special_tokens=True)\n",
    "    def generate(self, bin_ids, text_input, max_new_tokens=500, eos_token_id=None):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # 1. 时序编码（固定36长度）\n",
    "        ts_emb = self.process_ts(bin_ids.to(device))  # [1,36,dim]\n",
    "        \n",
    "        # 2. 文本输入处理\n",
    "        inputs = self.tokenizer(text_input, return_tensors='pt')\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        text_emb = self.llm.get_input_embeddings()(input_ids)  # [1,text_len,dim]\n",
    "        \n",
    "        # 3. 拼接完整输入（时序+文本）\n",
    "        inputs_embeds = torch.cat([ts_emb, text_emb], dim=1)  # [1,36+text_len,dim]\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(1, ts_emb.shape[1], device=device),\n",
    "            inputs.attention_mask.to(device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # 4. 禁用缓存的全量生成\n",
    "        generated_ids = input_ids.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = self.llm(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=False  # 关键修改\n",
    "            )\n",
    "            \n",
    "            next_token = outputs.logits[:, -1, :].argmax(-1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "            \n",
    "            # 更新输入（包含所有历史token）\n",
    "            new_emb = self.llm.get_input_embeddings()(next_token.unsqueeze(-1))\n",
    "            inputs_embeds = torch.cat([inputs_embeds, new_emb], dim=1)  # 逐步增长\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=device)], dim=1)\n",
    "            \n",
    "            if eos_token_id is not None and next_token.item() == eos_token_id:\n",
    "                break\n",
    "        \n",
    "        return self.tokenizer.decode(generated_ids[:, input_ids.shape[1]:][0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7633.8, 7361.0, 7136.0, 6840.9, 6624.9, 6481.7, 6426.5, 6431.3, 6466.9, 6616.1, 6686.2, 6963.8, 7234.6, 7545.8, 7819.5, 8102.2, 8342.4, 8582.6, 8781.7, 8959.6, 9173.4, 9426.9, 9619.0, 9802.6, 10005.1, 10136.4, 10342.1, 10521.1, 10716.8, 10942.5, 10901.9, 10754.9, 10724.1, 10498.2, 10434.6, 10252.5, 10187.4, 9940.8, 9693.8, 9558.5, 9444.0, 9253.6, 8946.8, 8820.5, 8444.4, 8325.2, 8043.2, 7845.5]\n",
      "tensor([[2167, 2163, 2159, 2155, 2151, 2149, 2148, 2148, 2149, 2151, 2152, 2157,\n",
      "         2161, 2166, 2170, 2174, 2178, 2182, 2185, 2188, 2191, 2195, 2198, 2201,\n",
      "         2204, 2206, 2209, 2212, 2215, 2219, 2218, 2216, 2215, 2212, 2211, 2208,\n",
      "         2207, 2203, 2199, 2197, 2195, 2192, 2188, 2186, 2180, 2178, 2174, 2170]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果:  0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "    \n",
    "# 1. 准备输入数据\n",
    "x = {\n",
    "        \"instruction\": \"The historical load data is: 7633.8,7361.0,7136.0,6840.9,6624.9,6481.7,6426.5,6431.3,6466.9,6616.1,6686.2,6963.8,7234.6,7545.8,7819.5,8102.2,8342.4,8582.6,8781.7,8959.6,9173.4,9426.9,9619.0,9802.6,10005.1,10136.4,10342.1,10521.1,10716.8,10942.5,10901.9,10754.9,10724.1,10498.2,10434.6,10252.5,10187.4,9940.8,9693.8,9558.5,9444.0,9253.6,8946.8,8820.5,8444.4,8325.2,8043.2,7845.5\",\n",
    "        \"input\": \"Based on the historical load data, please predict the load consumption in the next day. The region for prediction is NSW. The start date of historical data was on 2019-1-2 that is Weekday, and it is not a public holiday. The data frequency is 30 minutes per point. Historical data covers 1 day. The date of prediction is on 2019-1-3 that is Weekday, and it is not a public holiday. Weather of the start date: the minimum temperature is 294.54; the maximum temperature is 302.04; the humidity is 64.0; the pressure is 1013.0.  Weather of the prediction date: the minimum temperature is 293.03; the maximum temperature is 306.13; the humidity is 69.0; the pressure is 1013.0. There is no suitable news for long-term effect on future load consumption.In 2019-01-02 18:59:00, the news that had the Short-Term Effect on Today's Load Consumption is that Tropical cyclone Penny has reformed in the Coral Sea with weather forecasters predicting the storm will head back towards the east coast. The region that may be influenced is QLD. The rationality is that The presence of a tropical cyclone could lead to short-term fluctuations in load consumption as areas prepare for potential power outages or increased usage from emergency services and storm preparations. However, actual consumption might be reduced if outages occur and interrupt supply.In 2019-01-02 20:27:00, the news that had the Real-Time Direct Effect on Today's Load Consumption is that Darwin has sweated through its driest December since 1991, the Bureau of Meteorology has revealed. The region that may be influenced is WA. The rationality is that Given the dry conditions, there may be a higher usage of air conditioning and cooling systems leading to an immediate increase in electricity load consumption. The news report corresponds with the timeframe for possible real-time effects.\",\n",
    "        \"output\": \"7667.5,7422.3,7222.8,6955.2,6726.2,6619.1,6569.4,6529.2,6522.6,6625.2,6721.6,6987.9,7206.5,7525.7,7877.8,8157.3,8338.1,8538.9,8596.9,8756.0,8877.0,8986.0,9161.8,9276.8,9477.4,9576.9,9614.9,9747.6,9893.6,10106.7,10268.2,10440.9,10540.1,10449.6,10354.6,10127.3,9999.4,9855.6,9656.4,9553.0,9499.1,9297.6,9002.2,8845.9,8633.8,8438.8,8193.0,7961.8\"\n",
    "}\n",
    "\n",
    "# 2. 初始化tokenizer和配置\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-1.5B-Instruct')\n",
    "config = TimeLLMConfig(\n",
    "    tokenizer_class=\"MeanScaleQuantileBins\",\n",
    "    tokenizer_kwargs={\"low_limit\": -15.0, \"high_limit\": 15.0},\n",
    "    prediction_length=48,\n",
    "    n_tokens=4096,\n",
    "    query_len=36,\n",
    "    llm_name=\"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    ")\n",
    "\n",
    "# 3. 数据预处理\n",
    "# 时序数据编码\n",
    "ts_values = [float(v) for v in x[\"instruction\"].split(\":\")[1].strip().split(\",\")]\n",
    "print(ts_values)\n",
    "bin_tokenizer = config.create_tokenizer()\n",
    "bin_ids, _, scale = bin_tokenizer.context_input_transform(torch.tensor([ts_values]))\n",
    "print(bin_ids)\n",
    "\n",
    "# 文本token化\n",
    "input_text = f\"{x['input']}\\nanswer:\"\n",
    "\n",
    "# 4. 加载模型（假设模型已训练保存）\n",
    "model = TimeLLMModel.from_pretrained(\"/opt/tiger/dyf/final_model\").to('cuda')\n",
    "# print(model.llm.lm_head.weight.mean())\n",
    "# first_layer = model.llm.model.layers[0] \n",
    "# print(first_layer.mlp.up_proj.weight.mean())\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        bin_ids=bin_ids,\n",
    "        text_input=input_text,\n",
    "        max_new_tokens=500,\n",
    "        eos_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "print(\"预测结果:\", generated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有保存的键: ['alignment.0.bias', 'alignment.0.weight', 'alignment.2.bias', 'alignment.2.weight', 'bin_embed.weight', 'llm.lm_head.weight', 'llm.model.embed_tokens.weight', 'llm.model.layers.0.input_layernorm.weight', 'llm.model.layers.0.mlp.down_proj.weight', 'llm.model.layers.0.mlp.gate_proj.weight', 'llm.model.layers.0.mlp.up_proj.weight', 'llm.model.layers.0.post_attention_layernorm.weight', 'llm.model.layers.0.self_attn.k_proj.bias', 'llm.model.layers.0.self_attn.k_proj.weight', 'llm.model.layers.0.self_attn.o_proj.weight', 'llm.model.layers.0.self_attn.q_proj.bias', 'llm.model.layers.0.self_attn.q_proj.weight', 'llm.model.layers.0.self_attn.v_proj.bias', 'llm.model.layers.0.self_attn.v_proj.weight', 'llm.model.layers.1.input_layernorm.weight', 'llm.model.layers.1.mlp.down_proj.weight', 'llm.model.layers.1.mlp.gate_proj.weight', 'llm.model.layers.1.mlp.up_proj.weight', 'llm.model.layers.1.post_attention_layernorm.weight', 'llm.model.layers.1.self_attn.k_proj.bias', 'llm.model.layers.1.self_attn.k_proj.weight', 'llm.model.layers.1.self_attn.o_proj.weight', 'llm.model.layers.1.self_attn.q_proj.bias', 'llm.model.layers.1.self_attn.q_proj.weight', 'llm.model.layers.1.self_attn.v_proj.bias', 'llm.model.layers.1.self_attn.v_proj.weight', 'llm.model.layers.10.input_layernorm.weight', 'llm.model.layers.10.mlp.down_proj.weight', 'llm.model.layers.10.mlp.gate_proj.weight', 'llm.model.layers.10.mlp.up_proj.weight', 'llm.model.layers.10.post_attention_layernorm.weight', 'llm.model.layers.10.self_attn.k_proj.bias', 'llm.model.layers.10.self_attn.k_proj.weight', 'llm.model.layers.10.self_attn.o_proj.weight', 'llm.model.layers.10.self_attn.q_proj.bias', 'llm.model.layers.10.self_attn.q_proj.weight', 'llm.model.layers.10.self_attn.v_proj.bias', 'llm.model.layers.10.self_attn.v_proj.weight', 'llm.model.layers.11.input_layernorm.weight', 'llm.model.layers.11.mlp.down_proj.weight', 'llm.model.layers.11.mlp.gate_proj.weight', 'llm.model.layers.11.mlp.up_proj.weight', 'llm.model.layers.11.post_attention_layernorm.weight', 'llm.model.layers.11.self_attn.k_proj.bias', 'llm.model.layers.11.self_attn.k_proj.weight', 'llm.model.layers.11.self_attn.o_proj.weight', 'llm.model.layers.11.self_attn.q_proj.bias', 'llm.model.layers.11.self_attn.q_proj.weight', 'llm.model.layers.11.self_attn.v_proj.bias', 'llm.model.layers.11.self_attn.v_proj.weight', 'llm.model.layers.12.input_layernorm.weight', 'llm.model.layers.12.mlp.down_proj.weight', 'llm.model.layers.12.mlp.gate_proj.weight', 'llm.model.layers.12.mlp.up_proj.weight', 'llm.model.layers.12.post_attention_layernorm.weight', 'llm.model.layers.12.self_attn.k_proj.bias', 'llm.model.layers.12.self_attn.k_proj.weight', 'llm.model.layers.12.self_attn.o_proj.weight', 'llm.model.layers.12.self_attn.q_proj.bias', 'llm.model.layers.12.self_attn.q_proj.weight', 'llm.model.layers.12.self_attn.v_proj.bias', 'llm.model.layers.12.self_attn.v_proj.weight', 'llm.model.layers.13.input_layernorm.weight', 'llm.model.layers.13.mlp.down_proj.weight', 'llm.model.layers.13.mlp.gate_proj.weight', 'llm.model.layers.13.mlp.up_proj.weight', 'llm.model.layers.13.post_attention_layernorm.weight', 'llm.model.layers.13.self_attn.k_proj.bias', 'llm.model.layers.13.self_attn.k_proj.weight', 'llm.model.layers.13.self_attn.o_proj.weight', 'llm.model.layers.13.self_attn.q_proj.bias', 'llm.model.layers.13.self_attn.q_proj.weight', 'llm.model.layers.13.self_attn.v_proj.bias', 'llm.model.layers.13.self_attn.v_proj.weight', 'llm.model.layers.14.input_layernorm.weight', 'llm.model.layers.14.mlp.down_proj.weight', 'llm.model.layers.14.mlp.gate_proj.weight', 'llm.model.layers.14.mlp.up_proj.weight', 'llm.model.layers.14.post_attention_layernorm.weight', 'llm.model.layers.14.self_attn.k_proj.bias', 'llm.model.layers.14.self_attn.k_proj.weight', 'llm.model.layers.14.self_attn.o_proj.weight', 'llm.model.layers.14.self_attn.q_proj.bias', 'llm.model.layers.14.self_attn.q_proj.weight', 'llm.model.layers.14.self_attn.v_proj.bias', 'llm.model.layers.14.self_attn.v_proj.weight', 'llm.model.layers.15.input_layernorm.weight', 'llm.model.layers.15.mlp.down_proj.weight', 'llm.model.layers.15.mlp.gate_proj.weight', 'llm.model.layers.15.mlp.up_proj.weight', 'llm.model.layers.15.post_attention_layernorm.weight', 'llm.model.layers.15.self_attn.k_proj.bias', 'llm.model.layers.15.self_attn.k_proj.weight', 'llm.model.layers.15.self_attn.o_proj.weight', 'llm.model.layers.15.self_attn.q_proj.bias', 'llm.model.layers.15.self_attn.q_proj.weight', 'llm.model.layers.15.self_attn.v_proj.bias', 'llm.model.layers.15.self_attn.v_proj.weight', 'llm.model.layers.16.input_layernorm.weight', 'llm.model.layers.16.mlp.down_proj.weight', 'llm.model.layers.16.mlp.gate_proj.weight', 'llm.model.layers.16.mlp.up_proj.weight', 'llm.model.layers.16.post_attention_layernorm.weight', 'llm.model.layers.16.self_attn.k_proj.bias', 'llm.model.layers.16.self_attn.k_proj.weight', 'llm.model.layers.16.self_attn.o_proj.weight', 'llm.model.layers.16.self_attn.q_proj.bias', 'llm.model.layers.16.self_attn.q_proj.weight', 'llm.model.layers.16.self_attn.v_proj.bias', 'llm.model.layers.16.self_attn.v_proj.weight', 'llm.model.layers.17.input_layernorm.weight', 'llm.model.layers.17.mlp.down_proj.weight', 'llm.model.layers.17.mlp.gate_proj.weight', 'llm.model.layers.17.mlp.up_proj.weight', 'llm.model.layers.17.post_attention_layernorm.weight', 'llm.model.layers.17.self_attn.k_proj.bias', 'llm.model.layers.17.self_attn.k_proj.weight', 'llm.model.layers.17.self_attn.o_proj.weight', 'llm.model.layers.17.self_attn.q_proj.bias', 'llm.model.layers.17.self_attn.q_proj.weight', 'llm.model.layers.17.self_attn.v_proj.bias', 'llm.model.layers.17.self_attn.v_proj.weight', 'llm.model.layers.18.input_layernorm.weight', 'llm.model.layers.18.mlp.down_proj.weight', 'llm.model.layers.18.mlp.gate_proj.weight', 'llm.model.layers.18.mlp.up_proj.weight', 'llm.model.layers.18.post_attention_layernorm.weight', 'llm.model.layers.18.self_attn.k_proj.bias', 'llm.model.layers.18.self_attn.k_proj.weight', 'llm.model.layers.18.self_attn.o_proj.weight', 'llm.model.layers.18.self_attn.q_proj.bias', 'llm.model.layers.18.self_attn.q_proj.weight', 'llm.model.layers.18.self_attn.v_proj.bias', 'llm.model.layers.18.self_attn.v_proj.weight', 'llm.model.layers.19.input_layernorm.weight', 'llm.model.layers.19.mlp.down_proj.weight', 'llm.model.layers.19.mlp.gate_proj.weight', 'llm.model.layers.19.mlp.up_proj.weight', 'llm.model.layers.19.post_attention_layernorm.weight', 'llm.model.layers.19.self_attn.k_proj.bias', 'llm.model.layers.19.self_attn.k_proj.weight', 'llm.model.layers.19.self_attn.o_proj.weight', 'llm.model.layers.19.self_attn.q_proj.bias', 'llm.model.layers.19.self_attn.q_proj.weight', 'llm.model.layers.19.self_attn.v_proj.bias', 'llm.model.layers.19.self_attn.v_proj.weight', 'llm.model.layers.2.input_layernorm.weight', 'llm.model.layers.2.mlp.down_proj.weight', 'llm.model.layers.2.mlp.gate_proj.weight', 'llm.model.layers.2.mlp.up_proj.weight', 'llm.model.layers.2.post_attention_layernorm.weight', 'llm.model.layers.2.self_attn.k_proj.bias', 'llm.model.layers.2.self_attn.k_proj.weight', 'llm.model.layers.2.self_attn.o_proj.weight', 'llm.model.layers.2.self_attn.q_proj.bias', 'llm.model.layers.2.self_attn.q_proj.weight', 'llm.model.layers.2.self_attn.v_proj.bias', 'llm.model.layers.2.self_attn.v_proj.weight', 'llm.model.layers.20.input_layernorm.weight', 'llm.model.layers.20.mlp.down_proj.weight', 'llm.model.layers.20.mlp.gate_proj.weight', 'llm.model.layers.20.mlp.up_proj.weight', 'llm.model.layers.20.post_attention_layernorm.weight', 'llm.model.layers.20.self_attn.k_proj.bias', 'llm.model.layers.20.self_attn.k_proj.weight', 'llm.model.layers.20.self_attn.o_proj.weight', 'llm.model.layers.20.self_attn.q_proj.bias', 'llm.model.layers.20.self_attn.q_proj.weight', 'llm.model.layers.20.self_attn.v_proj.bias', 'llm.model.layers.20.self_attn.v_proj.weight', 'llm.model.layers.21.input_layernorm.weight', 'llm.model.layers.21.mlp.down_proj.weight', 'llm.model.layers.21.mlp.gate_proj.weight', 'llm.model.layers.21.mlp.up_proj.weight', 'llm.model.layers.21.post_attention_layernorm.weight', 'llm.model.layers.21.self_attn.k_proj.bias', 'llm.model.layers.21.self_attn.k_proj.weight', 'llm.model.layers.21.self_attn.o_proj.weight', 'llm.model.layers.21.self_attn.q_proj.bias', 'llm.model.layers.21.self_attn.q_proj.weight', 'llm.model.layers.21.self_attn.v_proj.bias', 'llm.model.layers.21.self_attn.v_proj.weight', 'llm.model.layers.22.input_layernorm.weight', 'llm.model.layers.22.mlp.down_proj.weight', 'llm.model.layers.22.mlp.gate_proj.weight', 'llm.model.layers.22.mlp.up_proj.weight', 'llm.model.layers.22.post_attention_layernorm.weight', 'llm.model.layers.22.self_attn.k_proj.bias', 'llm.model.layers.22.self_attn.k_proj.weight', 'llm.model.layers.22.self_attn.o_proj.weight', 'llm.model.layers.22.self_attn.q_proj.bias', 'llm.model.layers.22.self_attn.q_proj.weight', 'llm.model.layers.22.self_attn.v_proj.bias', 'llm.model.layers.22.self_attn.v_proj.weight', 'llm.model.layers.23.input_layernorm.weight', 'llm.model.layers.23.mlp.down_proj.weight', 'llm.model.layers.23.mlp.gate_proj.weight', 'llm.model.layers.23.mlp.up_proj.weight', 'llm.model.layers.23.post_attention_layernorm.weight', 'llm.model.layers.23.self_attn.k_proj.bias', 'llm.model.layers.23.self_attn.k_proj.weight', 'llm.model.layers.23.self_attn.o_proj.weight', 'llm.model.layers.23.self_attn.q_proj.bias', 'llm.model.layers.23.self_attn.q_proj.weight', 'llm.model.layers.23.self_attn.v_proj.bias', 'llm.model.layers.23.self_attn.v_proj.weight', 'llm.model.layers.24.input_layernorm.weight', 'llm.model.layers.24.mlp.down_proj.weight', 'llm.model.layers.24.mlp.gate_proj.weight', 'llm.model.layers.24.mlp.up_proj.weight', 'llm.model.layers.24.post_attention_layernorm.weight', 'llm.model.layers.24.self_attn.k_proj.bias', 'llm.model.layers.24.self_attn.k_proj.weight', 'llm.model.layers.24.self_attn.o_proj.weight', 'llm.model.layers.24.self_attn.q_proj.bias', 'llm.model.layers.24.self_attn.q_proj.weight', 'llm.model.layers.24.self_attn.v_proj.bias', 'llm.model.layers.24.self_attn.v_proj.weight', 'llm.model.layers.25.input_layernorm.weight', 'llm.model.layers.25.mlp.down_proj.weight', 'llm.model.layers.25.mlp.gate_proj.weight', 'llm.model.layers.25.mlp.up_proj.weight', 'llm.model.layers.25.post_attention_layernorm.weight', 'llm.model.layers.25.self_attn.k_proj.bias', 'llm.model.layers.25.self_attn.k_proj.weight', 'llm.model.layers.25.self_attn.o_proj.weight', 'llm.model.layers.25.self_attn.q_proj.bias', 'llm.model.layers.25.self_attn.q_proj.weight', 'llm.model.layers.25.self_attn.v_proj.bias', 'llm.model.layers.25.self_attn.v_proj.weight', 'llm.model.layers.26.input_layernorm.weight', 'llm.model.layers.26.mlp.down_proj.weight', 'llm.model.layers.26.mlp.gate_proj.weight', 'llm.model.layers.26.mlp.up_proj.weight', 'llm.model.layers.26.post_attention_layernorm.weight', 'llm.model.layers.26.self_attn.k_proj.bias', 'llm.model.layers.26.self_attn.k_proj.weight', 'llm.model.layers.26.self_attn.o_proj.weight', 'llm.model.layers.26.self_attn.q_proj.bias', 'llm.model.layers.26.self_attn.q_proj.weight', 'llm.model.layers.26.self_attn.v_proj.bias', 'llm.model.layers.26.self_attn.v_proj.weight', 'llm.model.layers.27.input_layernorm.weight', 'llm.model.layers.27.mlp.down_proj.weight', 'llm.model.layers.27.mlp.gate_proj.weight', 'llm.model.layers.27.mlp.up_proj.weight', 'llm.model.layers.27.post_attention_layernorm.weight', 'llm.model.layers.27.self_attn.k_proj.bias', 'llm.model.layers.27.self_attn.k_proj.weight', 'llm.model.layers.27.self_attn.o_proj.weight', 'llm.model.layers.27.self_attn.q_proj.bias', 'llm.model.layers.27.self_attn.q_proj.weight', 'llm.model.layers.27.self_attn.v_proj.bias', 'llm.model.layers.27.self_attn.v_proj.weight', 'llm.model.layers.3.input_layernorm.weight', 'llm.model.layers.3.mlp.down_proj.weight', 'llm.model.layers.3.mlp.gate_proj.weight', 'llm.model.layers.3.mlp.up_proj.weight', 'llm.model.layers.3.post_attention_layernorm.weight', 'llm.model.layers.3.self_attn.k_proj.bias', 'llm.model.layers.3.self_attn.k_proj.weight', 'llm.model.layers.3.self_attn.o_proj.weight', 'llm.model.layers.3.self_attn.q_proj.bias', 'llm.model.layers.3.self_attn.q_proj.weight', 'llm.model.layers.3.self_attn.v_proj.bias', 'llm.model.layers.3.self_attn.v_proj.weight', 'llm.model.layers.4.input_layernorm.weight', 'llm.model.layers.4.mlp.down_proj.weight', 'llm.model.layers.4.mlp.gate_proj.weight', 'llm.model.layers.4.mlp.up_proj.weight', 'llm.model.layers.4.post_attention_layernorm.weight', 'llm.model.layers.4.self_attn.k_proj.bias', 'llm.model.layers.4.self_attn.k_proj.weight', 'llm.model.layers.4.self_attn.o_proj.weight', 'llm.model.layers.4.self_attn.q_proj.bias', 'llm.model.layers.4.self_attn.q_proj.weight', 'llm.model.layers.4.self_attn.v_proj.bias', 'llm.model.layers.4.self_attn.v_proj.weight', 'llm.model.layers.5.input_layernorm.weight', 'llm.model.layers.5.mlp.down_proj.weight', 'llm.model.layers.5.mlp.gate_proj.weight', 'llm.model.layers.5.mlp.up_proj.weight', 'llm.model.layers.5.post_attention_layernorm.weight', 'llm.model.layers.5.self_attn.k_proj.bias', 'llm.model.layers.5.self_attn.k_proj.weight', 'llm.model.layers.5.self_attn.o_proj.weight', 'llm.model.layers.5.self_attn.q_proj.bias', 'llm.model.layers.5.self_attn.q_proj.weight', 'llm.model.layers.5.self_attn.v_proj.bias', 'llm.model.layers.5.self_attn.v_proj.weight', 'llm.model.layers.6.input_layernorm.weight', 'llm.model.layers.6.mlp.down_proj.weight', 'llm.model.layers.6.mlp.gate_proj.weight', 'llm.model.layers.6.mlp.up_proj.weight', 'llm.model.layers.6.post_attention_layernorm.weight', 'llm.model.layers.6.self_attn.k_proj.bias', 'llm.model.layers.6.self_attn.k_proj.weight', 'llm.model.layers.6.self_attn.o_proj.weight', 'llm.model.layers.6.self_attn.q_proj.bias', 'llm.model.layers.6.self_attn.q_proj.weight', 'llm.model.layers.6.self_attn.v_proj.bias', 'llm.model.layers.6.self_attn.v_proj.weight', 'llm.model.layers.7.input_layernorm.weight', 'llm.model.layers.7.mlp.down_proj.weight', 'llm.model.layers.7.mlp.gate_proj.weight', 'llm.model.layers.7.mlp.up_proj.weight', 'llm.model.layers.7.post_attention_layernorm.weight', 'llm.model.layers.7.self_attn.k_proj.bias', 'llm.model.layers.7.self_attn.k_proj.weight', 'llm.model.layers.7.self_attn.o_proj.weight', 'llm.model.layers.7.self_attn.q_proj.bias', 'llm.model.layers.7.self_attn.q_proj.weight', 'llm.model.layers.7.self_attn.v_proj.bias', 'llm.model.layers.7.self_attn.v_proj.weight', 'llm.model.layers.8.input_layernorm.weight', 'llm.model.layers.8.mlp.down_proj.weight', 'llm.model.layers.8.mlp.gate_proj.weight', 'llm.model.layers.8.mlp.up_proj.weight', 'llm.model.layers.8.post_attention_layernorm.weight', 'llm.model.layers.8.self_attn.k_proj.bias', 'llm.model.layers.8.self_attn.k_proj.weight', 'llm.model.layers.8.self_attn.o_proj.weight', 'llm.model.layers.8.self_attn.q_proj.bias', 'llm.model.layers.8.self_attn.q_proj.weight', 'llm.model.layers.8.self_attn.v_proj.bias', 'llm.model.layers.8.self_attn.v_proj.weight', 'llm.model.layers.9.input_layernorm.weight', 'llm.model.layers.9.mlp.down_proj.weight', 'llm.model.layers.9.mlp.gate_proj.weight', 'llm.model.layers.9.mlp.up_proj.weight', 'llm.model.layers.9.post_attention_layernorm.weight', 'llm.model.layers.9.self_attn.k_proj.bias', 'llm.model.layers.9.self_attn.k_proj.weight', 'llm.model.layers.9.self_attn.o_proj.weight', 'llm.model.layers.9.self_attn.q_proj.bias', 'llm.model.layers.9.self_attn.q_proj.weight', 'llm.model.layers.9.self_attn.v_proj.bias', 'llm.model.layers.9.self_attn.v_proj.weight', 'llm.model.norm.weight', 'query_attn.attn.in_proj_bias', 'query_attn.attn.in_proj_weight', 'query_attn.attn.out_proj.bias', 'query_attn.attn.out_proj.weight', 'query_attn.query']\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "with safe_open(\"/opt/tiger/dyf/final_model/model.safetensors\", framework=\"pt\") as f:\n",
    "    print(\"所有保存的键:\", list(f.keys()))  # 检查完整权重列表\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_attn.q_proj.weight: shape=torch.Size([1536, 1536]), mean=0.000055\n",
      "self_attn.q_proj.bias: shape=torch.Size([1536]), mean=0.056833\n",
      "self_attn.k_proj.weight: shape=torch.Size([256, 1536]), mean=-0.000031\n",
      "self_attn.k_proj.bias: shape=torch.Size([256]), mean=5.038458\n",
      "self_attn.v_proj.weight: shape=torch.Size([256, 1536]), mean=0.000093\n",
      "self_attn.v_proj.bias: shape=torch.Size([256]), mean=0.010747\n",
      "self_attn.o_proj.weight: shape=torch.Size([1536, 1536]), mean=0.000023\n",
      "mlp.gate_proj.weight: shape=torch.Size([8960, 1536]), mean=-0.000040\n",
      "mlp.up_proj.weight: shape=torch.Size([8960, 1536]), mean=0.000009\n",
      "mlp.down_proj.weight: shape=torch.Size([1536, 8960]), mean=-0.000007\n",
      "input_layernorm.weight: shape=torch.Size([1536]), mean=0.385370\n",
      "post_attention_layernorm.weight: shape=torch.Size([1536]), mean=0.472133\n",
      "输出头权重的均值: -4.026186798000708e-05\n",
      "llm.lm_head.weight 的均值: -3.999471664428711e-05\n",
      "mlp 的均值: 8.821487426757812e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    device_map=\"auto\"           # 自动分配设备\n",
    ")\n",
    "first_layer = qwen.model.layers[0]\n",
    "\n",
    "for name, param in first_layer.named_parameters():\n",
    "    print(f\"{name}: shape={param.shape}, mean={param.mean().item():.6f}\")\n",
    "\n",
    "lm_head_weight = qwen.lm_head.weight\n",
    "mean_value = torch.mean(lm_head_weight).item()\n",
    "print(\"输出头权重的均值:\", mean_value)\n",
    "\n",
    "# 打开模型文件\n",
    "with safe_open(\"/opt/tiger/dyf/final_model/model.safetensors\", framework=\"pt\") as f:\n",
    "    \n",
    "    # 加载 llm.lm_head.weight 权重\n",
    "    lm_head_weight = f.get_tensor(\"llm.lm_head.weight\")\n",
    "    llm_model_layers_0_mlp_up_proj_weight = f.get_tensor('llm.model.layers.0.mlp.up_proj.weight')\n",
    "    \n",
    "    # 计算均值\n",
    "    mean_value = torch.mean(lm_head_weight).item()\n",
    "    mean_value2 = torch.mean(llm_model_layers_0_mlp_up_proj_weight).item()\n",
    "    print(\"llm.lm_head.weight 的均值:\", mean_value)\n",
    "    print(\"mlp 的均值:\", mean_value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "98d64b5a-48d4-4cd6-86a4-97e6b4bbb009",
  "filePath": "/opt/tiger/dyf/eval/eval.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

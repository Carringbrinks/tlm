{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9aedecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PretrainedConfig, AutoConfig, PreTrainedModel\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "class TimeLLMConfig(PretrainedConfig):\n",
    "    \"\"\"\n",
    "    HF-Compatible Configuration for TimeLLM Model\n",
    "    (Non-dataclass version with full PretrainedConfig integration)\n",
    "    \"\"\"\n",
    "    model_type = \"time_llm\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer_kwargs: Dict[str, Any] = None,\n",
    "        prediction_length: int = 24,\n",
    "        n_tokens: int = 4096,\n",
    "        query_len: int = 36,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # 必须调用父类初始化（处理HF标准参数）\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # 核心自定义参数\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs or {}\n",
    "        self.prediction_length = prediction_length\n",
    "        self.n_tokens = n_tokens\n",
    "        self.query_len = query_len\n",
    "\n",
    "    def create_tokenizer(self) -> 'TimeLLMTokenizer':\n",
    "        \n",
    "        return MeanScaleQuantileBins(**self.tokenizer_kwargs, config=self)\n",
    "\n",
    "class TimeLLMTokenizer:\n",
    "    \"\"\"Base class for time series tokenizers\"\"\"\n",
    "    def context_input_transform(self, context: torch.Tensor) -> Tuple:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def label_input_transform(self, label: torch.Tensor, tokenizer_state: Any) -> Tuple:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def output_transform(self, samples: torch.Tensor, tokenizer_state: Any) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MeanScaleQuantileBins(TimeLLMTokenizer):\n",
    "    \"\"\"Quantile-based binning tokenizer for time series\"\"\"\n",
    "    def __init__(self, low_limit: float, high_limit: float, config: TimeLLMConfig):\n",
    "        self.config = config\n",
    "        self.centers = torch.linspace(\n",
    "            low_limit, high_limit,\n",
    "            config.n_tokens - 1,\n",
    "        )\n",
    "        self.boundaries = torch.concat([\n",
    "            torch.tensor([-1e20]),\n",
    "            (self.centers[1:] + self.centers[:-1]) / 2,\n",
    "            torch.tensor([1e20])\n",
    "        ])\n",
    "\n",
    "    def _input_transform(self, context: torch.Tensor, scale: Optional[torch.Tensor] = None):\n",
    "        context = context.float()\n",
    "        attention_mask = ~torch.isnan(context)\n",
    "\n",
    "        if scale is None:\n",
    "            scale = torch.nansum(torch.abs(context) * attention_mask, dim=-1) / \\\n",
    "                   torch.nansum(attention_mask, dim=-1)\n",
    "            scale[~(scale > 0)] = 1.0\n",
    "\n",
    "        scaled_context = context / scale.unsqueeze(-1)\n",
    "        token_ids = torch.bucketize(scaled_context, self.boundaries,right=True).clamp(0, self.config.n_tokens - 1)  # 直接使用完整token空间\n",
    "        \n",
    "        return token_ids, attention_mask, scale\n",
    "\n",
    "    def context_input_transform(self, context: torch.Tensor):\n",
    "        # if context.shape[-1] > self.config.context_length:\n",
    "        #     context = context[..., -self.config.context_length:]\n",
    "            \n",
    "        token_ids, attention_mask, scale = self._input_transform(context)\n",
    "            \n",
    "        return token_ids, attention_mask, scale\n",
    "\n",
    "    def output_transform(self, samples: torch.Tensor, scale: torch.Tensor):\n",
    "        \"\"\"将模型输出的token索引转换为实际数值\"\"\"\n",
    "        indices = torch.clamp(\n",
    "            samples,  # 直接使用原始token索引\n",
    "            min=0,\n",
    "            max=len(self.centers)-1\n",
    "        )\n",
    "        return self.centers[indices] * scale.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "\n",
    "class QueryAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, latent_len: int, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.latent_len = latent_len\n",
    "        # 可学习的Query矩阵 (L×D)\n",
    "        self.query = nn.Parameter(torch.randn(latent_len, embed_dim))\n",
    "        # 多头注意力\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        输入: \n",
    "          x: [B, T, D] \n",
    "          key_padding_mask: [B, T]（可选）\n",
    "        输出: \n",
    "          [B, L, D]\n",
    "        \"\"\"\n",
    "        # 扩展Query为[B, L, D]\n",
    "        queries = self.query.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        \n",
    "        # 注意力计算\n",
    "        attn_out, _ = self.attn(\n",
    "            query=queries,  # [B, L, D]\n",
    "            key=x,          # [B, T, D]\n",
    "            value=x,        # [B, T, D]\n",
    "            key_padding_mask=key_padding_mask  # 忽略padding部分\n",
    "        )\n",
    "        return attn_out  # [B, L, D]\n",
    "\n",
    "class TimeLLMModel(PreTrainedModel):\n",
    "    config_class = TimeLLMConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        # 2. 文本处理模块\n",
    "        self.llm_config = AutoConfig.from_pretrained(config.llm_name, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_name, trust_remote_code=True)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            config.llm_name,\n",
    "            config=self.llm_config,\n",
    "            # device_map='auto', # 必须用关键字参数\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm_dim = self.llm_config.hidden_size  # 模型的隐藏维度\n",
    "\n",
    "        # 1. 时序处理模块\n",
    "        self.bin_embed = nn.Embedding(config.n_tokens, self.llm_dim)\n",
    "        self.query_attn = QueryAttention(self.llm_dim, config.query_len)  # Query向量\n",
    "        self.alignment = nn.Sequential(\n",
    "            nn.Linear(self.llm_dim, self.llm_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.llm_dim, self.llm_dim)\n",
    "        )\n",
    "        \n",
    "    def process_ts(self, bin_id):\n",
    "        \"\"\"时序数据编码：分箱 -> Embedding -> 注意力筛选 -> 投影\"\"\"\n",
    "        # ts_data: [batch, context_length]\n",
    "        \n",
    "        bin_embedding = self.bin_embed(bin_id)  # [batch, ctx_len, dim]\n",
    "        bin_feat = self.query_attn(bin_embedding)\n",
    "        \n",
    "        return self.alignment(bin_feat)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bin_ids, labels=None, scales=None):\n",
    "        \"\"\"\n",
    "        关键设计：\n",
    "        - 输入格式：[时序Token][文本Token][预测值Token]\n",
    "        - 训练时：通过错位labels实现自回归\n",
    "        \"\"\"\n",
    "        # 1. 处理时序数据\n",
    "        ts_emb = self.process_ts(bin_ids) \n",
    "        \n",
    "        # 2. 获取文本嵌入\n",
    "        text_emb = self.llm.get_input_embeddings()(input_ids)  \n",
    "        \n",
    "        # 3. 拼接输入 [时序][文本]\n",
    "        inputs_embeds = torch.cat([ts_emb, text_emb], dim=1)  \n",
    "        \n",
    "        # 5. 通过LLM生成（自回归）\n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels  \n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, ts_data, text_input, max_new_tokens=48):\n",
    "        # 初始化输入\n",
    "        input_ids = self.tokenizer(text_input, return_tensors='pt').input_ids\n",
    "        ts_emb = self.process_ts(ts_data)\n",
    "        \n",
    "        # 自回归循环\n",
    "        for _ in range(max_new_tokens):\n",
    "            text_emb = self.llm.get_input_embeddings()(input_ids)\n",
    "            inputs_embeds = torch.cat([ts_emb, text_emb], dim=1)\n",
    "            \n",
    "            outputs = self.llm(inputs_embeds=inputs_embeds)\n",
    "            next_token = outputs.logits[:, -1, :].argmax(-1)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=-1)\n",
    "            \n",
    "        return input_ids[:, -max_new_tokens:]  \n",
    "\n",
    "    \n",
    "class TimeLLMPipeline:\n",
    "    def __init__(self, config, model_path=None):\n",
    "        self.config = config\n",
    "        self.tokenizer = config.create_tokenizer()\n",
    "        self.model = TimeLLMModel(config)\n",
    "        \n",
    "        if model_path:\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def preprocess(self, raw_ts: List[float], text: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"将原始数据转换为模型输入\"\"\"\n",
    "        # 1. 时序分箱\n",
    "        bin_ids, _, scale = self.tokenizer.context_input_transform(\n",
    "            torch.tensor([raw_ts])\n",
    "        )\n",
    "        \n",
    "        # 2. 文本token化\n",
    "        text_enc = self.model.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            # max_length=self.config.context_length\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"bin_ids\": bin_ids,\n",
    "            \"input_ids\": text_enc.input_ids,\n",
    "            \"attention_mask\": text_enc.attention_mask,\n",
    "            \"scale\": scale  # 保留用于逆变换\n",
    "        }\n",
    "\n",
    "    def postprocess(self, pred_tokens: torch.Tensor, scale: float) -> List[float]:\n",
    "        \"\"\"将模型输出转换为原始数值\"\"\"\n",
    "        return self.tokenizer.output_transform(pred_tokens, scale).tolist()\n",
    "\n",
    "    def predict(self, raw_ts: List[float], text: str) -> List[float]:\n",
    "        \"\"\"端到端预测\"\"\"\n",
    "        inputs = self.preprocess(raw_ts, text)\n",
    "        pred_tokens = self.model.generate(\n",
    "            inputs[\"bin_ids\"], \n",
    "            text_input=text,\n",
    "            max_new_tokens=self.config.prediction_length\n",
    "        )\n",
    "        return self.postprocess(pred_tokens, inputs[\"scale\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3aa7d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TimeLLMConfig(\n",
    "        tokenizer_class=\"MeanScaleQuantileBins\",\n",
    "        tokenizer_kwargs={\"low_limit\": -15.0, \"high_limit\": 15.0},\n",
    "        prediction_length=48,\n",
    "        n_tokens=4096,\n",
    "        query_len=36,\n",
    "        llm_name=\"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8673d155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['instruction', 'input', 'output'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['instruction', 'input', 'output'],\n",
       "     num_rows: 3655\n",
       " }))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_test = load_dataset('json', data_files='/opt/tiger/dyf/data/AULF_test_data_2021.json', split='train')\n",
    "dataset_train = load_dataset('json', data_files='/opt/tiger/dyf/data/AULF_train_data_2019-2020.json', split='train')\n",
    "dataset_test, dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70a26181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'The historical load data is: 1189.4,1147.1,1136.2,1143.0,1148.7,1156.9,1161.1,1136.7,1128.3,1145.7,1131.3,1168.6,1225.8,1300.6,1394.4,1468.7,1540.0,1550.1,1488.8,1456.8,1422.4,1372.1,1342.9,1294.8,1247.7,1221.3,1189.8,1203.8,1196.2,1210.2,1230.6,1260.5,1311.1,1349.7,1423.7,1487.1,1549.6,1570.6,1565.4,1536.9,1500.6,1495.2,1409.5,1370.1,1345.4,1291.9,1254.8,1192.7', 'input': 'Based on the historical load data, please predict the load consumption in the next day. The region for prediction is TAS. The start date of historical data was on 2021-8-3 that is Weekday, and it is not a public holiday. The data frequency is 30 minutes per point. Historical data covers 1 day. The date of prediction is on 2021-8-4 that is Weekday, and it is not a public holiday. Weather of the start date: the minimum temperature is 279.71; the maximum temperature is 285.83; the humidity is 85.0; the pressure is 1003.0.  Weather of the prediction date: the minimum temperature is 280.54; the maximum temperature is 286.47; the humidity is 74.0; the pressure is 1007.0. ', 'output': '1165.5,1144.3,1134.8,1109.1,1100.4,1083.2,1095.0,1098.2,1093.5,1107.9,1118.9,1171.8,1232.8,1311.7,1413.4,1499.7,1536.3,1511.4,1491.0,1454.6,1450.2,1401.1,1352.4,1307.0,1204.5,1210.6,1160.9,1171.2,1193.2,1262.4,1284.1,1296.7,1312.4,1347.6,1382.5,1443.3,1496.9,1494.1,1493.2,1478.3,1448.2,1432.3,1403.3,1366.8,1325.1,1273.8,1230.6,1186.8'}\n"
     ]
    }
   ],
   "source": [
    "d = dataset_test[0]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_tokenizer = config.create_tokenizer()\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(config.llm_name)\n",
    "def process_example(example):\n",
    "    # 时序数据分箱\n",
    "    ts_values = [float(x) for x in example[\"instruction\"].split(\":\")[1].strip().split(\",\")]\n",
    "    bin_ids, _, scale = bin_tokenizer.context_input_transform(torch.tensor([ts_values]))\n",
    "    \n",
    "    # 构造输入输出文本\n",
    "    input_text = f\"{example['input']}\\nanswer:\"\n",
    "    target_text = example[\"output\"]\n",
    "\n",
    "    # Tokenize（自动处理截断和填充）\n",
    "    tokenized = llm_tokenizer(\n",
    "        input_text + target_text,\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # 计算loss mask\n",
    "    input_len = len(llm_tokenizer(input_text, add_special_tokens=False)[\"input_ids\"])\n",
    "    labels = [-100] * config.query_len + tokenized[\"input_ids\"].copy()  # 前16是时序部分\n",
    "    labels[config.query_len : config.query_len + input_len] = [-100] * input_len  # 标记instruction\n",
    "\n",
    "\n",
    "    return {\n",
    "    \"bin_ids\": bin_ids[0].tolist(),\n",
    "    \"input_ids\": tokenized[\"input_ids\"],\n",
    "    \"attention_mask\": [1] * (config.query_len + len(tokenized[\"input_ids\"])),  # 时序+文本\n",
    "    \"labels\": labels,\n",
    "    \"scale\": scale.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_ids [2172, 2167, 2166, 2167, 2167, 2168, 2169, 2166, 2165, 2167, 2166, 2169, 2175, 2183, 2193, 2201, 2208, 2209, 2203, 2199, 2196, 2191, 2187, 2182, 2178, 2175, 2172, 2173, 2172, 2174, 2176, 2179, 2184, 2188, 2196, 2202, 2209, 2211, 2211, 2208, 2204, 2203, 2194, 2190, 2188, 2182, 2178, 2172]\n",
      "input_ids [28715, 389, 279, 13656, 2795, 821, 11, 4486, 7023, 279, 2795, 15293, 304, 279, 1790, 1899, 13, 576, 5537, 369, 19639, 374, 91288, 13, 576, 1191, 2400, 315, 13656, 821, 572, 389, 220, 17, 15, 17, 16, 12, 23, 12, 18, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 576, 821, 11639, 374, 220, 18, 15, 4420, 817, 1459, 13, 40043, 821, 14521, 220, 16, 1899, 13, 576, 2400, 315, 19639, 374, 389, 220, 17, 15, 17, 16, 12, 23, 12, 19, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 22629, 315, 279, 1191, 2400, 25, 279, 8028, 9315, 374, 220, 17, 22, 24, 13, 22, 16, 26, 279, 7192, 9315, 374, 220, 17, 23, 20, 13, 23, 18, 26, 279, 37093, 374, 220, 23, 20, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 15, 18, 13, 15, 13, 220, 22629, 315, 279, 19639, 2400, 25, 279, 8028, 9315, 374, 220, 17, 23, 15, 13, 20, 19, 26, 279, 7192, 9315, 374, 220, 17, 23, 21, 13, 19, 22, 26, 279, 37093, 374, 220, 22, 19, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 15, 22, 13, 15, 13, 715, 9217, 25, 16, 16, 21, 20, 13, 20, 11, 16, 16, 19, 19, 13, 18, 11, 16, 16, 18, 19, 13, 23, 11, 16, 16, 15, 24, 13, 16, 11, 16, 16, 15, 15, 13, 19, 11, 16, 15, 23, 18, 13, 17, 11, 16, 15, 24, 20, 13, 15, 11, 16, 15, 24, 23, 13, 17, 11, 16, 15, 24, 18, 13, 20, 11, 16, 16, 15, 22, 13, 24, 11, 16, 16, 16, 23, 13, 24, 11, 16, 16, 22, 16, 13, 23, 11, 16, 17, 18, 17, 13, 23, 11, 16, 18, 16, 16, 13, 22, 11, 16, 19, 16, 18, 13, 19, 11, 16, 19, 24, 24, 13, 22, 11, 16, 20, 18, 21, 13, 18, 11, 16, 20, 16, 16, 13, 19, 11, 16, 19, 24, 16, 13, 15, 11, 16, 19, 20, 19, 13, 21, 11, 16, 19, 20, 15, 13, 17, 11, 16, 19, 15, 16, 13, 16, 11, 16, 18, 20, 17, 13, 19, 11, 16, 18, 15, 22, 13, 15, 11, 16, 17, 15, 19, 13, 20, 11, 16, 17, 16, 15, 13, 21, 11, 16, 16, 21, 15, 13, 24, 11, 16, 16, 22, 16, 13, 17, 11, 16, 16, 24, 18, 13, 17, 11, 16, 17, 21, 17, 13, 19, 11, 16, 17, 23, 19, 13, 16, 11, 16, 17, 24, 21, 13, 22, 11, 16, 18, 16, 17, 13, 19, 11, 16, 18, 19, 22, 13, 21, 11, 16, 18, 23, 17, 13, 20, 11, 16, 19, 19, 18, 13, 18, 11, 16, 19, 24, 21, 13, 24, 11, 16, 19, 24, 19, 13, 16, 11, 16, 19, 24, 18, 13, 17, 11, 16, 19, 22, 23, 13, 18, 11, 16, 19, 19, 23, 13, 17, 11, 16, 19, 18, 17, 13, 18, 11, 16, 19, 15, 18, 13, 18, 11, 16, 18, 21, 21, 13, 23, 11, 16, 18, 17, 20, 13, 16, 11, 16, 17, 22, 18, 13, 23, 11, 16, 17, 18, 15, 13, 21, 11, 16, 16, 23, 21, 13, 23]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 16, 21, 20, 13, 20, 11, 16, 16, 19, 19, 13, 18, 11, 16, 16, 18, 19, 13, 23, 11, 16, 16, 15, 24, 13, 16, 11, 16, 16, 15, 15, 13, 19, 11, 16, 15, 23, 18, 13, 17, 11, 16, 15, 24, 20, 13, 15, 11, 16, 15, 24, 23, 13, 17, 11, 16, 15, 24, 18, 13, 20, 11, 16, 16, 15, 22, 13, 24, 11, 16, 16, 16, 23, 13, 24, 11, 16, 16, 22, 16, 13, 23, 11, 16, 17, 18, 17, 13, 23, 11, 16, 18, 16, 16, 13, 22, 11, 16, 19, 16, 18, 13, 19, 11, 16, 19, 24, 24, 13, 22, 11, 16, 20, 18, 21, 13, 18, 11, 16, 20, 16, 16, 13, 19, 11, 16, 19, 24, 16, 13, 15, 11, 16, 19, 20, 19, 13, 21, 11, 16, 19, 20, 15, 13, 17, 11, 16, 19, 15, 16, 13, 16, 11, 16, 18, 20, 17, 13, 19, 11, 16, 18, 15, 22, 13, 15, 11, 16, 17, 15, 19, 13, 20, 11, 16, 17, 16, 15, 13, 21, 11, 16, 16, 21, 15, 13, 24, 11, 16, 16, 22, 16, 13, 17, 11, 16, 16, 24, 18, 13, 17, 11, 16, 17, 21, 17, 13, 19, 11, 16, 17, 23, 19, 13, 16, 11, 16, 17, 24, 21, 13, 22, 11, 16, 18, 16, 17, 13, 19, 11, 16, 18, 19, 22, 13, 21, 11, 16, 18, 23, 17, 13, 20, 11, 16, 19, 19, 18, 13, 18, 11, 16, 19, 24, 21, 13, 24, 11, 16, 19, 24, 19, 13, 16, 11, 16, 19, 24, 18, 13, 17, 11, 16, 19, 22, 23, 13, 18, 11, 16, 19, 19, 23, 13, 17, 11, 16, 19, 18, 17, 13, 18, 11, 16, 19, 15, 18, 13, 18, 11, 16, 18, 21, 21, 13, 23, 11, 16, 18, 17, 20, 13, 16, 11, 16, 17, 22, 18, 13, 23, 11, 16, 17, 18, 15, 13, 21, 11, 16, 16, 23, 21, 13, 23]\n",
      "scale 1313.8499755859375\n"
     ]
    }
   ],
   "source": [
    "a = process_example(d)\n",
    "for key in a:\n",
    "    print(key, a[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputid 539\n",
      "label: 575\n",
      "attention 575\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print('inputid', len(a['input_ids']))\n",
    "print('label:', len(a['labels']))\n",
    "print('attention', len(a['attention_mask']))\n",
    "print(len(a['bin_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "source": [
    "deco = llm_tokenizer.decode([11])\n",
    "print(deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e752b2decd4611bdb102eacf04881b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset_test.map(\n",
    "        process_example,\n",
    "        batched=False,\n",
    "        remove_columns=dataset_test.column_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['bin_ids', 'input_ids', 'attention_mask', 'labels', 'scale'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "bin_ids: [2172, 2167, 2166, 2167, 2167, 2168, 2169, 2166, 2165, 2167, 2166, 2169, 2175, 2183, 2193, 2201, 2208, 2209, 2203, 2199, 2196, 2191, 2187, 2182, 2178, 2175, 2172, 2173, 2172, 2174, 2176, 2179, 2184, 2188, 2196, 2202, 2209, 2211, 2211, 2208, 2204, 2203, 2194, 2190, 2188, 2182, 2178, 2172]\n",
      "Length of bin_ids: 48\n",
      "input_ids: [28715, 389, 279, 13656, 2795, 821, 11, 4486, 7023, 279, 2795, 15293, 304, 279, 1790, 1899, 13, 576, 5537, 369, 19639, 374, 91288, 13, 576, 1191, 2400, 315, 13656, 821, 572, 389, 220, 17, 15, 17, 16, 12, 23, 12, 18, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 576, 821, 11639, 374, 220, 18, 15, 4420, 817, 1459, 13, 40043, 821, 14521, 220, 16, 1899, 13, 576, 2400, 315, 19639, 374, 389, 220, 17, 15, 17, 16, 12, 23, 12, 19, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 22629, 315, 279, 1191, 2400, 25, 279, 8028, 9315, 374, 220, 17, 22, 24, 13, 22, 16, 26, 279, 7192, 9315, 374, 220, 17, 23, 20, 13, 23, 18, 26, 279, 37093, 374, 220, 23, 20, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 15, 18, 13, 15, 13, 220, 22629, 315, 279, 19639, 2400, 25, 279, 8028, 9315, 374, 220, 17, 23, 15, 13, 20, 19, 26, 279, 7192, 9315, 374, 220, 17, 23, 21, 13, 19, 22, 26, 279, 37093, 374, 220, 22, 19, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 15, 22, 13, 15, 13, 715, 9217, 25, 16, 16, 21, 20, 13, 20, 11, 16, 16, 19, 19, 13, 18, 11, 16, 16, 18, 19, 13, 23, 11, 16, 16, 15, 24, 13, 16, 11, 16, 16, 15, 15, 13, 19, 11, 16, 15, 23, 18, 13, 17, 11, 16, 15, 24, 20, 13, 15, 11, 16, 15, 24, 23, 13, 17, 11, 16, 15, 24, 18, 13, 20, 11, 16, 16, 15, 22, 13, 24, 11, 16, 16, 16, 23, 13, 24, 11, 16, 16, 22, 16, 13, 23, 11, 16, 17, 18, 17, 13, 23, 11, 16, 18, 16, 16, 13, 22, 11, 16, 19, 16, 18, 13, 19, 11, 16, 19, 24, 24, 13, 22, 11, 16, 20, 18, 21, 13, 18, 11, 16, 20, 16, 16, 13, 19, 11, 16, 19, 24, 16, 13, 15, 11, 16, 19, 20, 19, 13, 21, 11, 16, 19, 20, 15, 13, 17, 11, 16, 19, 15, 16, 13, 16, 11, 16, 18, 20, 17, 13, 19, 11, 16, 18, 15, 22, 13, 15, 11, 16, 17, 15, 19, 13, 20, 11, 16, 17, 16, 15, 13, 21, 11, 16, 16, 21, 15, 13, 24, 11, 16, 16, 22, 16, 13, 17, 11, 16, 16, 24, 18, 13, 17, 11, 16, 17, 21, 17, 13, 19, 11, 16, 17, 23, 19, 13, 16, 11, 16, 17, 24, 21, 13, 22, 11, 16, 18, 16, 17, 13, 19, 11, 16, 18, 19, 22, 13, 21, 11, 16, 18, 23, 17, 13, 20, 11, 16, 19, 19, 18, 13, 18, 11, 16, 19, 24, 21, 13, 24, 11, 16, 19, 24, 19, 13, 16, 11, 16, 19, 24, 18, 13, 17, 11, 16, 19, 22, 23, 13, 18, 11, 16, 19, 19, 23, 13, 17, 11, 16, 19, 18, 17, 13, 18, 11, 16, 19, 15, 18, 13, 18, 11, 16, 18, 21, 21, 13, 23, 11, 16, 18, 17, 20, 13, 16, 11, 16, 17, 22, 18, 13, 23, 11, 16, 17, 18, 15, 13, 21, 11, 16, 16, 23, 21, 13, 23]\n",
      "Length of input_ids: 539\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of attention_mask: 575\n",
      "labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 16, 21, 20, 13, 20, 11, 16, 16, 19, 19, 13, 18, 11, 16, 16, 18, 19, 13, 23, 11, 16, 16, 15, 24, 13, 16, 11, 16, 16, 15, 15, 13, 19, 11, 16, 15, 23, 18, 13, 17, 11, 16, 15, 24, 20, 13, 15, 11, 16, 15, 24, 23, 13, 17, 11, 16, 15, 24, 18, 13, 20, 11, 16, 16, 15, 22, 13, 24, 11, 16, 16, 16, 23, 13, 24, 11, 16, 16, 22, 16, 13, 23, 11, 16, 17, 18, 17, 13, 23, 11, 16, 18, 16, 16, 13, 22, 11, 16, 19, 16, 18, 13, 19, 11, 16, 19, 24, 24, 13, 22, 11, 16, 20, 18, 21, 13, 18, 11, 16, 20, 16, 16, 13, 19, 11, 16, 19, 24, 16, 13, 15, 11, 16, 19, 20, 19, 13, 21, 11, 16, 19, 20, 15, 13, 17, 11, 16, 19, 15, 16, 13, 16, 11, 16, 18, 20, 17, 13, 19, 11, 16, 18, 15, 22, 13, 15, 11, 16, 17, 15, 19, 13, 20, 11, 16, 17, 16, 15, 13, 21, 11, 16, 16, 21, 15, 13, 24, 11, 16, 16, 22, 16, 13, 17, 11, 16, 16, 24, 18, 13, 17, 11, 16, 17, 21, 17, 13, 19, 11, 16, 17, 23, 19, 13, 16, 11, 16, 17, 24, 21, 13, 22, 11, 16, 18, 16, 17, 13, 19, 11, 16, 18, 19, 22, 13, 21, 11, 16, 18, 23, 17, 13, 20, 11, 16, 19, 19, 18, 13, 18, 11, 16, 19, 24, 21, 13, 24, 11, 16, 19, 24, 19, 13, 16, 11, 16, 19, 24, 18, 13, 17, 11, 16, 19, 22, 23, 13, 18, 11, 16, 19, 19, 23, 13, 17, 11, 16, 19, 18, 17, 13, 18, 11, 16, 19, 15, 18, 13, 18, 11, 16, 18, 21, 21, 13, 23, 11, 16, 18, 17, 20, 13, 16, 11, 16, 17, 22, 18, 13, 23, 11, 16, 17, 18, 15, 13, 21, 11, 16, 16, 23, 21, 13, 23]\n",
      "Length of labels: 575\n",
      "scale: 1313.8499755859375\n",
      "\n",
      "\n",
      "Sample 1:\n",
      "bin_ids: [2186, 2184, 2182, 2180, 2177, 2172, 2168, 2165, 2164, 2164, 2166, 2170, 2176, 2187, 2194, 2199, 2199, 2197, 2192, 2184, 2171, 2171, 2167, 2163, 2159, 2159, 2159, 2158, 2159, 2160, 2164, 2169, 2175, 2183, 2195, 2206, 2221, 2228, 2227, 2222, 2219, 2215, 2211, 2207, 2202, 2198, 2191, 2189]\n",
      "Length of bin_ids: 48\n",
      "input_ids: [28715, 389, 279, 13656, 2795, 821, 11, 4486, 7023, 279, 2795, 15293, 304, 279, 1790, 1899, 13, 576, 5537, 369, 19639, 374, 37908, 13, 576, 1191, 2400, 315, 13656, 821, 572, 389, 220, 17, 15, 17, 16, 12, 23, 12, 16, 22, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 576, 821, 11639, 374, 220, 18, 15, 4420, 817, 1459, 13, 40043, 821, 14521, 220, 16, 1899, 13, 576, 2400, 315, 19639, 374, 389, 220, 17, 15, 17, 16, 12, 23, 12, 16, 23, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 22629, 315, 279, 1191, 2400, 25, 279, 8028, 9315, 374, 220, 17, 23, 15, 13, 19, 21, 26, 279, 7192, 9315, 374, 220, 17, 24, 16, 13, 15, 21, 26, 279, 37093, 374, 220, 22, 18, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 17, 21, 13, 15, 13, 220, 22629, 315, 279, 19639, 2400, 25, 279, 8028, 9315, 374, 220, 17, 23, 15, 13, 15, 23, 26, 279, 7192, 9315, 374, 220, 17, 24, 18, 13, 17, 26, 279, 37093, 374, 220, 23, 23, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 17, 19, 13, 15, 13, 715, 9217, 25, 23, 17, 21, 19, 13, 24, 11, 23, 17, 17, 19, 13, 22, 11, 23, 16, 15, 18, 13, 16, 11, 22, 24, 18, 18, 13, 17, 11, 22, 21, 23, 21, 13, 24, 11, 22, 19, 17, 17, 13, 18, 11, 22, 17, 16, 16, 13, 16, 11, 21, 24, 23, 16, 13, 23, 11, 21, 24, 19, 22, 13, 16, 11, 21, 24, 17, 21, 13, 18, 11, 22, 15, 16, 17, 13, 19, 11, 22, 18, 16, 18, 13, 22, 11, 22, 22, 15, 20, 13, 17, 11, 23, 18, 15, 22, 13, 18, 11, 23, 22, 15, 18, 13, 16, 11, 24, 15, 18, 23, 13, 16, 11, 24, 15, 21, 24, 13, 19, 11, 24, 15, 17, 15, 13, 24, 11, 23, 22, 20, 19, 13, 21, 11, 23, 17, 23, 21, 13, 24, 11, 22, 22, 22, 23, 13, 17, 11, 22, 18, 23, 19, 13, 24, 11, 22, 15, 21, 17, 13, 22, 11, 21, 23, 16, 23, 13, 24, 11, 21, 22, 18, 23, 13, 17, 11, 21, 22, 18, 22, 13, 16, 11, 21, 21, 20, 19, 13, 16, 11, 21, 20, 23, 23, 13, 15, 11, 21, 20, 24, 15, 13, 18, 11, 21, 20, 24, 18, 13, 24, 11, 21, 21, 22, 20, 13, 17, 11, 21, 23, 22, 18, 13, 18, 11, 22, 17, 21, 18, 13, 20, 11, 22, 22, 17, 16, 13, 16, 11, 23, 17, 23, 15, 13, 15, 11, 23, 24, 21, 15, 13, 15, 11, 24, 22, 20, 22, 13, 21, 11, 16, 15, 17, 21, 22, 13, 24, 11, 16, 15, 17, 19, 16, 13, 22, 11, 16, 15, 15, 20, 19, 13, 23, 11, 24, 22, 23, 19, 13, 17, 11, 24, 21, 17, 18, 13, 19, 11, 24, 18, 20, 15, 13, 21, 11, 24, 16, 15, 20, 13, 19, 11, 23, 23, 16, 19, 13, 21, 11, 23, 22, 20, 22, 13, 23, 11, 23, 20, 23, 22, 13, 18, 11, 23, 19, 23, 18, 13, 16]\n",
      "Length of input_ids: 543\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of attention_mask: 579\n",
      "labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 23, 17, 21, 19, 13, 24, 11, 23, 17, 17, 19, 13, 22, 11, 23, 16, 15, 18, 13, 16, 11, 22, 24, 18, 18, 13, 17, 11, 22, 21, 23, 21, 13, 24, 11, 22, 19, 17, 17, 13, 18, 11, 22, 17, 16, 16, 13, 16, 11, 21, 24, 23, 16, 13, 23, 11, 21, 24, 19, 22, 13, 16, 11, 21, 24, 17, 21, 13, 18, 11, 22, 15, 16, 17, 13, 19, 11, 22, 18, 16, 18, 13, 22, 11, 22, 22, 15, 20, 13, 17, 11, 23, 18, 15, 22, 13, 18, 11, 23, 22, 15, 18, 13, 16, 11, 24, 15, 18, 23, 13, 16, 11, 24, 15, 21, 24, 13, 19, 11, 24, 15, 17, 15, 13, 24, 11, 23, 22, 20, 19, 13, 21, 11, 23, 17, 23, 21, 13, 24, 11, 22, 22, 22, 23, 13, 17, 11, 22, 18, 23, 19, 13, 24, 11, 22, 15, 21, 17, 13, 22, 11, 21, 23, 16, 23, 13, 24, 11, 21, 22, 18, 23, 13, 17, 11, 21, 22, 18, 22, 13, 16, 11, 21, 21, 20, 19, 13, 16, 11, 21, 20, 23, 23, 13, 15, 11, 21, 20, 24, 15, 13, 18, 11, 21, 20, 24, 18, 13, 24, 11, 21, 21, 22, 20, 13, 17, 11, 21, 23, 22, 18, 13, 18, 11, 22, 17, 21, 18, 13, 20, 11, 22, 22, 17, 16, 13, 16, 11, 23, 17, 23, 15, 13, 15, 11, 23, 24, 21, 15, 13, 15, 11, 24, 22, 20, 22, 13, 21, 11, 16, 15, 17, 21, 22, 13, 24, 11, 16, 15, 17, 19, 16, 13, 22, 11, 16, 15, 15, 20, 19, 13, 23, 11, 24, 22, 23, 19, 13, 17, 11, 24, 21, 17, 18, 13, 19, 11, 24, 18, 20, 15, 13, 21, 11, 24, 16, 15, 20, 13, 19, 11, 23, 23, 16, 19, 13, 21, 11, 23, 22, 20, 22, 13, 23, 11, 23, 20, 23, 22, 13, 18, 11, 23, 19, 23, 18, 13, 16]\n",
      "Length of labels: 579\n",
      "scale: 7832.72509765625\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "bin_ids: [2178, 2175, 2172, 2166, 2163, 2160, 2160, 2159, 2159, 2160, 2161, 2163, 2164, 2167, 2168, 2171, 2172, 2174, 2176, 2176, 2175, 2176, 2177, 2178, 2179, 2181, 2185, 2188, 2191, 2193, 2197, 2203, 2208, 2210, 2215, 2218, 2219, 2218, 2217, 2216, 2212, 2207, 2201, 2197, 2191, 2189, 2186, 2181]\n",
      "Length of bin_ids: 48\n",
      "input_ids: [28715, 389, 279, 13656, 2795, 821, 11, 4486, 7023, 279, 2795, 15293, 304, 279, 1790, 1899, 13, 576, 5537, 369, 19639, 374, 37908, 13, 576, 1191, 2400, 315, 13656, 821, 572, 389, 220, 17, 15, 17, 16, 12, 17, 12, 17, 16, 429, 374, 47434, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 576, 821, 11639, 374, 220, 18, 15, 4420, 817, 1459, 13, 40043, 821, 14521, 220, 16, 1899, 13, 576, 2400, 315, 19639, 374, 389, 220, 17, 15, 17, 16, 12, 17, 12, 17, 17, 429, 374, 10348, 1292, 11, 323, 432, 374, 537, 264, 584, 13257, 13, 22629, 315, 279, 1191, 2400, 25, 279, 8028, 9315, 374, 220, 17, 24, 18, 13, 16, 26, 279, 7192, 9315, 374, 220, 18, 15, 15, 13, 24, 20, 26, 279, 37093, 374, 220, 22, 18, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 15, 21, 13, 15, 13, 220, 22629, 315, 279, 19639, 2400, 25, 279, 8028, 9315, 374, 220, 17, 24, 17, 13, 17, 20, 26, 279, 7192, 9315, 374, 220, 17, 24, 18, 13, 21, 17, 26, 279, 37093, 374, 220, 22, 22, 13, 15, 26, 279, 7262, 374, 220, 16, 15, 16, 17, 13, 15, 13, 1913, 220, 17, 15, 17, 16, 12, 15, 17, 12, 17, 16, 220, 16, 23, 25, 15, 23, 25, 15, 15, 11, 279, 3669, 646, 2297, 279, 882, 4013, 38288, 4002, 429, 576, 50451, 15386, 9104, 4682, 1231, 23973, 2355, 5128, 323, 2990, 311, 12851, 10431, 12624, 11, 5961, 27887, 17728, 2795, 15293, 304, 1931, 7246, 624, 9217, 25, 22, 16, 22, 21, 13, 17, 11, 21, 24, 24, 24, 13, 17, 11, 21, 22, 24, 22, 13, 24, 11, 21, 21, 15, 19, 13, 24, 11, 21, 19, 17, 22, 13, 23, 11, 21, 18, 18, 17, 13, 24, 11, 21, 17, 20, 18, 13, 16, 11, 21, 17, 24, 24, 13, 17, 11, 21, 18, 20, 22, 13, 24, 11, 21, 20, 19, 19, 13, 24, 11, 21, 22, 21, 17, 13, 23, 11, 22, 17, 17, 18, 13, 16, 11, 22, 21, 21, 24, 13, 23, 11, 22, 24, 23, 23, 13, 22, 11, 23, 16, 20, 23, 13, 24, 11, 23, 17, 17, 20, 13, 23, 11, 23, 16, 23, 15, 13, 17, 11, 23, 16, 20, 23, 13, 24, 11, 23, 15, 19, 15, 13, 21, 11, 23, 15, 22, 24, 13, 23, 11, 23, 15, 20, 22, 13, 21, 11, 23, 16, 16, 16, 13, 19, 11, 23, 16, 24, 22, 13, 20, 11, 23, 17, 19, 23, 13, 15, 11, 23, 18, 21, 21, 13, 21, 11, 23, 20, 15, 24, 13, 23, 11, 23, 21, 24, 16, 13, 17, 11, 23, 23, 20, 24, 13, 15, 11, 23, 24, 21, 15, 13, 18, 11, 24, 16, 20, 15, 13, 20, 11, 24, 18, 24, 22, 13, 24, 11, 24, 21, 21, 19, 13, 16, 11, 24, 23, 22, 19, 13, 18, 11, 24, 24, 16, 24, 13, 19, 11, 16, 15, 15, 19, 18, 13, 22, 11, 24, 23, 17, 15, 13, 18, 11, 24, 21, 22, 22, 13, 21, 11, 24, 19, 23, 16, 13, 23, 11, 24, 16, 24, 23, 13, 23, 11, 24, 15, 24, 19, 13, 16, 11, 23, 23, 16, 22, 13, 17, 11, 23, 19, 24, 19, 13, 21, 11, 23, 17, 17, 15, 13, 15, 11, 23, 15, 23, 15, 13, 16, 11, 22, 23, 19, 23, 13, 15, 11, 22, 22, 15, 16, 13, 21, 11, 22, 20, 18, 21, 13, 16, 11, 22, 17, 24, 24, 13, 17]\n",
      "Length of input_ids: 596\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length of attention_mask: 632\n",
      "labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 22, 16, 22, 21, 13, 17, 11, 21, 24, 24, 24, 13, 17, 11, 21, 22, 24, 22, 13, 24, 11, 21, 21, 15, 19, 13, 24, 11, 21, 19, 17, 22, 13, 23, 11, 21, 18, 18, 17, 13, 24, 11, 21, 17, 20, 18, 13, 16, 11, 21, 17, 24, 24, 13, 17, 11, 21, 18, 20, 22, 13, 24, 11, 21, 20, 19, 19, 13, 24, 11, 21, 22, 21, 17, 13, 23, 11, 22, 17, 17, 18, 13, 16, 11, 22, 21, 21, 24, 13, 23, 11, 22, 24, 23, 23, 13, 22, 11, 23, 16, 20, 23, 13, 24, 11, 23, 17, 17, 20, 13, 23, 11, 23, 16, 23, 15, 13, 17, 11, 23, 16, 20, 23, 13, 24, 11, 23, 15, 19, 15, 13, 21, 11, 23, 15, 22, 24, 13, 23, 11, 23, 15, 20, 22, 13, 21, 11, 23, 16, 16, 16, 13, 19, 11, 23, 16, 24, 22, 13, 20, 11, 23, 17, 19, 23, 13, 15, 11, 23, 18, 21, 21, 13, 21, 11, 23, 20, 15, 24, 13, 23, 11, 23, 21, 24, 16, 13, 17, 11, 23, 23, 20, 24, 13, 15, 11, 23, 24, 21, 15, 13, 18, 11, 24, 16, 20, 15, 13, 20, 11, 24, 18, 24, 22, 13, 24, 11, 24, 21, 21, 19, 13, 16, 11, 24, 23, 22, 19, 13, 18, 11, 24, 24, 16, 24, 13, 19, 11, 16, 15, 15, 19, 18, 13, 22, 11, 24, 23, 17, 15, 13, 18, 11, 24, 21, 22, 22, 13, 21, 11, 24, 19, 23, 16, 13, 23, 11, 24, 16, 24, 23, 13, 23, 11, 24, 15, 24, 19, 13, 16, 11, 23, 23, 16, 22, 13, 17, 11, 23, 19, 24, 19, 13, 21, 11, 23, 17, 17, 15, 13, 15, 11, 23, 15, 23, 15, 13, 16, 11, 22, 23, 19, 23, 13, 15, 11, 22, 22, 15, 16, 13, 21, 11, 22, 20, 18, 21, 13, 16, 11, 22, 17, 24, 24, 13, 17]\n",
      "Length of labels: 632\n",
      "scale: 7510.345703125\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_sample(dataset, index):\n",
    "    sample = dataset[index]\n",
    "    for key, value in sample.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        if isinstance(value, list):\n",
    "            print(f\"Length of {key}: {len(value)}\")\n",
    "        elif isinstance(value, torch.Tensor):\n",
    "            print(f\"Shape of {key}: {value.shape}\")\n",
    "\n",
    "# 打印数据集中的前几个样本\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print_sample(processed_dataset, i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_text_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    total_len = config.query_len + max_text_len\n",
    "    \n",
    "    def pad_field(values, pad_value):\n",
    "        return torch.stack([\n",
    "            torch.cat([\n",
    "                torch.tensor(v, dtype=torch.long),\n",
    "                torch.full((total_len - len(v),), pad_value)\n",
    "            ]) for v in values\n",
    "        ])\n",
    "    \n",
    "    return {\n",
    "        \"bin_ids\": torch.stack([torch.tensor(x[\"bin_ids\"]) for x in batch]),\n",
    "        \"input_ids\": pad_field(\n",
    "            [x[\"input_ids\"] for x in batch], \n",
    "            llm_tokenizer.pad_token_id  # 使用传入的tokenizer\n",
    "        )[:, config.query_len:],\n",
    "        \"attention_mask\": pad_field([x[\"attention_mask\"] for x in batch], 0),\n",
    "        \"labels\": pad_field([x[\"labels\"] for x in batch], -100),\n",
    "        \"scales\": torch.tensor([x[\"scale\"] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_ids: tensor([[2172, 2167, 2166, 2167, 2167, 2168, 2169, 2166, 2165, 2167, 2166, 2169,\n",
      "         2175, 2183, 2193, 2201, 2208, 2209, 2203, 2199, 2196, 2191, 2187, 2182,\n",
      "         2178, 2175, 2172, 2173, 2172, 2174, 2176, 2179, 2184, 2188, 2196, 2202,\n",
      "         2209, 2211, 2211, 2208, 2204, 2203, 2194, 2190, 2188, 2182, 2178, 2172],\n",
      "        [2186, 2184, 2182, 2180, 2177, 2172, 2168, 2165, 2164, 2164, 2166, 2170,\n",
      "         2176, 2187, 2194, 2199, 2199, 2197, 2192, 2184, 2171, 2171, 2167, 2163,\n",
      "         2159, 2159, 2159, 2158, 2159, 2160, 2164, 2169, 2175, 2183, 2195, 2206,\n",
      "         2221, 2228, 2227, 2222, 2219, 2215, 2211, 2207, 2202, 2198, 2191, 2189],\n",
      "        [2178, 2175, 2172, 2166, 2163, 2160, 2160, 2159, 2159, 2160, 2161, 2163,\n",
      "         2164, 2167, 2168, 2171, 2172, 2174, 2176, 2176, 2175, 2176, 2177, 2178,\n",
      "         2179, 2181, 2185, 2188, 2191, 2193, 2197, 2203, 2208, 2210, 2215, 2218,\n",
      "         2219, 2218, 2217, 2216, 2212, 2207, 2201, 2197, 2191, 2189, 2186, 2181]])\n",
      "Shape of bin_ids: torch.Size([3, 48])\n",
      "input_ids: tensor([[    16,     12,     23,  ..., 151643, 151643, 151643],\n",
      "        [    16,     12,     23,  ..., 151643, 151643, 151643],\n",
      "        [    16,     12,     17,  ..., 151643, 151643, 151643]])\n",
      "Shape of input_ids: torch.Size([3, 596])\n",
      "attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Shape of attention_mask: torch.Size([3, 632])\n",
      "labels: tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ...,   24,   13,   17]])\n",
      "Shape of labels: torch.Size([3, 632])\n",
      "scales: tensor([1313.8500, 7832.7251, 7510.3457])\n",
      "Shape of scales: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经有一个预处理后的数据集 processed_dataset\n",
    "# 选择前几个样本作为测试批量数据\n",
    "batch = [processed_dataset[i] for i in range(3)]\n",
    "\n",
    "# 调用 collate_fn 函数\n",
    "collated_batch = collate_fn(batch)\n",
    "\n",
    "# 打印 collated_batch 的结构和内容\n",
    "for key, value in collated_batch.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"Shape of {key}: {value.shape}\")"
   ]
  }
 ],
 "metadata": {
  "fileId": "4c0a511a-02b0-41af-bb4b-ac640d261c19",
  "filePath": "/opt/tiger/dyf/train/exp.ipynb",
  "kernelspec": {
   "display_name": "tlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
